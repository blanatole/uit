#!/usr/bin/env python3
"""
Script Ä‘á»ƒ dá»± Ä‘oÃ¡n trÃªn vihallu-public-test.csv báº±ng best model
"""
import re, torch, json, argparse, sys
import pandas as pd
from pathlib import Path
from transformers import AutoModelForCausalLM, AutoTokenizer
from tqdm import tqdm

LABELS = ["no", "intrinsic", "extrinsic"]
PAT = re.compile(r"\b(no|intrinsic|extrinsic)\b", re.IGNORECASE)

def build_prompt(row):
    """XÃ¢y dá»±ng prompt cho inference"""
    return (
        "Báº¡n lÃ  bá»™ phÃ¢n loáº¡i áº£o giÃ¡c. Nhiá»‡m vá»¥: dá»±a trÃªn NGá»® Cáº¢NH vÃ  CÃ‚U TRáº¢ Lá»œI cá»§a trá»£ lÃ½, "
        "hÃ£y phÃ¢n loáº¡i duy nháº¥t má»™t nhÃ£n trong {no, intrinsic, extrinsic}.\n\n"
        "Äá»‹nh nghÄ©a:\n"
        "- no: cÃ¢u tráº£ lá»i Ä‘Æ°á»£c há»— trá»£ Ä‘áº§y Ä‘á»§ bá»Ÿi ngá»¯ cáº£nh.\n"
        "- intrinsic: mÃ¢u thuáº«n/sai lá»‡ch so vá»›i ngá»¯ cáº£nh.\n"
        "- extrinsic: Ä‘Æ°a thÃªm thÃ´ng tin khÃ´ng cÃ³ trong ngá»¯ cáº£nh vÃ  khÃ´ng thá»ƒ kiá»ƒm chá»©ng.\n\n"
        f"NGá»® Cáº¢NH:\n{row['context']}\n\n"
        f"CÃ‚U Há»I:\n{row['prompt']}\n\n"
        f"CÃ‚U TRáº¢ Lá»œI Cá»¦A TRá»¢ LÃ:\n{row['response']}\n\n"
        "Chá»‰ in Ä‘Ãºng má»™t nhÃ£n: no hoáº·c intrinsic hoáº·c extrinsic."
    )

@torch.inference_mode()
def predict_public_test(input_csv, output_csv, model_dir):
    """Dá»± Ä‘oÃ¡n trÃªn public test set"""
    print(f"ğŸ”„ Loading model from {model_dir}...")
    tok = AutoTokenizer.from_pretrained(model_dir, use_fast=True)
    model = AutoModelForCausalLM.from_pretrained(model_dir, device_map="auto", torch_dtype=torch.bfloat16)
    
    print(f"ğŸ“‚ Loading dataset from {input_csv}...")
    df = pd.read_csv(input_csv)
    
    print(f"ğŸ” Predicting on {len(df)} samples...")
    print("=" * 50)
    
    predictions = []
    pbar = tqdm(df.iterrows(), total=len(df), desc="ğŸ” Predicting", dynamic_ncols=True)
    
    for i, (_, row) in enumerate(pbar):
        msgs = [{"role": "user", "content": build_prompt(row)}]
        text = tok.apply_chat_template(msgs, tokenize=False, add_generation_prompt=True)
        inputs = tok(text, return_tensors="pt").to(model.device)
        
        out = model.generate(
            **inputs, 
            max_new_tokens=4, 
            do_sample=False, 
            eos_token_id=tok.eos_token_id,
            pad_token_id=tok.eos_token_id
        )
        
        dec = tok.decode(out[0][inputs["input_ids"].shape[1]:], skip_special_tokens=True)
        m = PAT.search(dec)
        y = m.group(1).lower() if m else "no"
        predictions.append(y)
        
        # Update progress bar
        pbar.set_postfix({'Pred': y, 'Sample': f'{i+1}/{len(df)}'})
    
    # Táº¡o output DataFrame
    df_output = pd.DataFrame({
        'id': df['id'],
        'predict_label': predictions
    })
    
    # LÆ°u káº¿t quáº£
    df_output.to_csv(output_csv, index=False)
    print(f"âœ… Saved predictions to {output_csv}")
    
    # Thá»‘ng kÃª
    print("\nğŸ“Š Prediction distribution:")
    pred_counts = pd.Series(predictions).value_counts()
    for label in LABELS:
        count = pred_counts.get(label, 0)
        pct = count / len(predictions) * 100
        print(f"  {label}: {count} ({pct:.1f}%)")

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Predict on vihallu-public-test.csv")
    parser.add_argument("--input_csv", type=str, default="data/vihallu-public-test.csv", help="Input CSV file")
    parser.add_argument("--output_csv", type=str, default="preds_vihallu_public_test.csv", help="Output CSV file")
    parser.add_argument("--model_dir", type=str, default="out/gemma-vihallu/best", help="Model directory")
    
    args = parser.parse_args()
    
    print(f"ğŸ” Predicting on: {args.input_csv}")
    print(f"ğŸ“ Using model: {args.model_dir}")
    print(f"ğŸ’¾ Output to: {args.output_csv}")
    
    predict_public_test(args.input_csv, args.output_csv, args.model_dir)

import re, torch, json, argparse, sys
import pandas as pd
from pathlib import Path
from transformers import AutoModelForCausalLM, AutoTokenizer
from sklearn.metrics import f1_score, accuracy_score, classification_report, confusion_matrix

LABELS = ["no", "intrinsic", "extrinsic"]
PAT = re.compile(r"\b(no|intrinsic|extrinsic)\b", re.IGNORECASE)


def build_prompt(row):
    return (
        "B·∫°n l√† b·ªô ph√¢n lo·∫°i ·∫£o gi√°c. Nhi·ªám v·ª•: d·ª±a tr√™n NG·ªÆ C·∫¢NH v√† C√ÇU TR·∫¢ L·ªúI c·ªßa tr·ª£ l√Ω, "
        "h√£y ph√¢n lo·∫°i duy nh·∫•t m·ªôt nh√£n trong {no, intrinsic, extrinsic}.\n\n"
        "ƒê·ªãnh nghƒ©a:\n"
        "- no: c√¢u tr·∫£ l·ªùi ƒë∆∞·ª£c h·ªó tr·ª£ ƒë·∫ßy ƒë·ªß b·ªüi ng·ªØ c·∫£nh.\n"
        "- intrinsic: m√¢u thu·∫´n/sai l·ªách so v·ªõi ng·ªØ c·∫£nh.\n"
        "- extrinsic: ƒë∆∞a th√™m th√¥ng tin kh√¥ng c√≥ trong ng·ªØ c·∫£nh v√† kh√¥ng th·ªÉ ki·ªÉm ch·ª©ng.\n\n"
        f"NG·ªÆ C·∫¢NH:\n{row['context']}\n\n"
        f"C√ÇU H·ªéI:\n{row['prompt']}\n\n"
        f"C√ÇU TR·∫¢ L·ªúI C·ª¶A TR·ª¢ L√ù:\n{row['response']}\n\n"
        "Ch·ªâ in ƒë√∫ng m·ªôt nh√£n: no ho·∫∑c intrinsic ho·∫∑c extrinsic."
    )


@torch.inference_mode()
def predict(df_path: str, model_dir: str, test_mode: bool = False):
    from tqdm import tqdm
    import time
    
    print(f"üîÑ Loading model from {model_dir}...")
    tok = AutoTokenizer.from_pretrained(model_dir, use_fast=True)
    model = AutoModelForCausalLM.from_pretrained(model_dir, device_map="auto", torch_dtype=torch.bfloat16)
    
    print(f"üìÇ Loading dataset from {df_path}...")
    df = pd.read_json(df_path, lines=True)
    
    # Test mode: ch·ªâ d√πng 25 samples
    if test_mode:
        df = df.head(25)
        print(f"üß™ TEST MODE: Using only {len(df)} samples")

    preds, gts = [], []
    print(f"üîç Evaluating {len(df)} samples...")
    print("=" * 50)
    
    # T·∫°o progress bar v·ªõi th√¥ng tin chi ti·∫øt h∆°n - force hi·ªÉn th·ªã
    pbar = tqdm(
        df.iterrows(),
        total=len(df),
        desc="üîç Evaluating",
        bar_format='{l_bar}{bar}| {n_fmt}/{total_fmt} [{elapsed}<{remaining}, {rate_fmt}]',
        disable=False,
        dynamic_ncols=True,
        ascii=True,
    )
    
    for i, (_, row) in enumerate(pbar):
        start_time = time.time()
        
        msgs = [{"role": "user", "content": build_prompt(row)}]
        text = tok.apply_chat_template(msgs, tokenize=False, add_generation_prompt=True)
        inputs = tok(text, return_tensors="pt").to(model.device)
        
        out = model.generate(
            **inputs, max_new_tokens=4, do_sample=False, eos_token_id=tok.eos_token_id
        )
        
        dec = tok.decode(out[0][inputs["input_ids"].shape[1]:], skip_special_tokens=True)
        m = PAT.search(dec)
        y = m.group(1).lower() if m else "no"
        preds.append(y)
        gts.append(row["label"].strip().lower())
        
        # C·∫≠p nh·∫≠t description v·ªõi th√¥ng tin th·ªùi gian th·ª±c
        elapsed = time.time() - start_time
        pbar.set_postfix({
            'Sample': f'{i+1}/{len(df)}',
            'Time': f'{elapsed:.2f}s',
            'Pred': y,
            'True': row["label"].strip().lower()
        })
        pbar.refresh()

    acc = accuracy_score(gts, preds)
    f1m = f1_score(gts, preds, average="macro", labels=LABELS, zero_division=0)
    print("Accuracy:", acc)
    print("Macro-F1:", f1m)
    print("Report:\n", classification_report(gts, preds, labels=LABELS, digits=4, zero_division=0))
    print("Confusion:\n", confusion_matrix(gts, preds, labels=LABELS))


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Evaluate Gemma model on validation/test set")
    parser.add_argument("--ckpt", type=str, required=True, help="Checkpoint directory (e.g., out/gemma-vihallu/checkpoint-1000)")
    parser.add_argument("--split", type=str, choices=["val", "test"], default="val", help="Dataset split to evaluate")
    parser.add_argument("--test_mode", action="store_true", help="Test mode: only evaluate on 25 samples")
    args = parser.parse_args()
    
    # Map split to file path
    split_files = {"val": "data/val.jsonl", "test": "data/test.jsonl"}
    df_path = split_files[args.split]
    
    print(f"üîç Evaluating checkpoint: {args.ckpt}")
    print(f"üìä Dataset: {args.split} ({df_path})")
    if args.test_mode:
        print("üß™ TEST MODE: Only evaluating on 25 samples")
    
    predict(df_path, args.ckpt, test_mode=args.test_mode)
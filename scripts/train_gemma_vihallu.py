import json, re
from dataclasses import dataclass
from typing import Dict, List
from pathlib import Path
import torch
from datasets import load_dataset
from transformers import (AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig,
                          TrainingArguments, Trainer, DataCollatorForLanguageModeling)
from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training

MODEL_NAME = "google/gemma-7b-it"
MAX_LEN = 2048


def _format_messages(row: Dict) -> List[Dict[str, str]]:
    user = (
        "B·∫°n l√† b·ªô ph√¢n lo·∫°i ·∫£o gi√°c. Nhi·ªám v·ª•: d·ª±a tr√™n NG·ªÆ C·∫¢NH v√† C√ÇU TR·∫¢ L·ªúI c·ªßa tr·ª£ l√Ω, "
        "h√£y ph√¢n lo·∫°i duy nh·∫•t m·ªôt nh√£n trong {no, intrinsic, extrinsic}.\n\n"
        "ƒê·ªãnh nghƒ©a:\n"
        "- no: c√¢u tr·∫£ l·ªùi ƒë∆∞·ª£c h·ªó tr·ª£ ƒë·∫ßy ƒë·ªß b·ªüi ng·ªØ c·∫£nh.\n"
        "- intrinsic: m√¢u thu·∫´n/sai l·ªách so v·ªõi ng·ªØ c·∫£nh (sai s·ªë li·ªáu, thu·ªôc t√≠nh, quan h·ªá...).\n"
        "- extrinsic: ƒë∆∞a th√™m th√¥ng tin kh√¥ng c√≥ trong ng·ªØ c·∫£nh v√† kh√¥ng th·ªÉ ki·ªÉm ch·ª©ng t·ª´ ng·ªØ c·∫£nh.\n\n"
        f"NG·ªÆ C·∫¢NH:\n{row['context']}\n\n"
        f"C√ÇU H·ªéI:\n{row['prompt']}\n\n"
        f"C√ÇU TR·∫¢ L·ªúI C·ª¶A TR·ª¢ L√ù:\n{row['response']}\n\n"
        "Ch·ªâ in ƒë√∫ng m·ªôt nh√£n: no ho·∫∑c intrinsic ho·∫∑c extrinsic."
    )
    assistant = row["label"].strip().lower()
    return [{"role": "user", "content": user}, {"role": "assistant", "content": assistant}]


def build_dataset(train_path: str, val_path: str, test_path: str, tokenizer: AutoTokenizer):
    ds = load_dataset("json", data_files={
        "train": train_path, "validation": val_path, "test": test_path
    })
    
    def to_text(x: Dict):
        messages = _format_messages(x)
        text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=False)
        return {"text": text}
    
    ds = ds.map(to_text)
    
    def tokenize_fn(examples):
        tokenized_inputs = tokenizer(
            examples["text"],
            max_length=MAX_LEN,
            truncation=True,
            padding="max_length",
            return_tensors="pt"
        )
        tokenized_inputs["labels"] = tokenized_inputs["input_ids"].clone()
        return tokenized_inputs
    
    ds_train = ds["train"].map(tokenize_fn, batched=True, remove_columns=ds["train"].column_names)
    ds_val = ds["validation"].map(tokenize_fn, batched=True, remove_columns=ds["validation"].column_names)
    
    return ds_train, ds_val


if __name__ == "__main__":
    import argparse
    parser = argparse.ArgumentParser()
    parser.add_argument("--test_mode", action="store_true", help="Test mode: 1 epoch, 50 train samples, 25 val samples")
    args = parser.parse_args()
    
    tok = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)
    tok.padding_side = "left"
    tok.pad_token = tok.eos_token

    # Test mode: ch·ªâ d√πng 50 m·∫´u train, 25 m·∫´u val ƒë·ªÉ test nhanh h∆°n
    if args.test_mode:
        print("üß™ TEST MODE: 1 epoch, 50 train samples, 25 val samples")
        ds_train_full, ds_val_full = build_dataset("data/train.jsonl", "data/val.jsonl", "data/test.jsonl", tok)
        ds_train = ds_train_full.select(range(50))
        ds_val = ds_val_full.select(range(25))
    else:
        ds_train, ds_val = build_dataset("data/train.jsonl", "data/val.jsonl", "data/test.jsonl", tok)

    bnb_cfg = BitsAndBytesConfig(
        load_in_4bit=True, bnb_4bit_quant_type="nf4",
        bnb_4bit_use_double_quant=True, bnb_4bit_compute_dtype=torch.bfloat16,
    )

    model = AutoModelForCausalLM.from_pretrained(
        MODEL_NAME, quantization_config=bnb_cfg, dtype=torch.bfloat16, device_map="auto"
    )
    
    # Ensure padding is defined for Trainer-internal tokenization if needed
    if getattr(model.config, "pad_token_id", None) is None and getattr(model.config, "eos_token_id", None) is not None:
        model.config.pad_token_id = model.config.eos_token_id
    
    # QLoRA: prepare for k-bit training (enable grads on inputs, cast norms, etc.)
    model = prepare_model_for_kbit_training(model, use_gradient_checkpointing=True)
    model.config.use_cache = False

    lora_cfg = LoraConfig(
        r=32, lora_alpha=64, target_modules=["q_proj","k_proj","v_proj","o_proj","gate_proj","up_proj","down_proj"],
        lora_dropout=0.05, bias="none", task_type="CAUSAL_LM"
    )

    # ƒêi·ªÅu ch·ªânh config cho test mode
    if args.test_mode:
        per_device_train_batch_size = 2  # Nh·ªè h∆°n cho test mode
        gradient_accumulation_steps = 2
        logging_steps = 5
        save_steps = 50
    else:
        per_device_train_batch_size = 4  # TƒÉng cho production
        gradient_accumulation_steps = 4
        logging_steps = 50
        save_steps = 500

    # Ki·ªÉm tra xem c√≥ checkpoint ƒë·ªÉ resume kh√¥ng
    has_checkpoint = Path("out/gemma-vihallu").exists() and any(Path("out/gemma-vihallu").glob("checkpoint-*"))
    
    args = TrainingArguments(
        output_dir="out/gemma-vihallu",
        num_train_epochs=1,  # Ch·ªâ train 1 epoch m·ªói l·∫ßn ch·∫°y
        per_device_train_batch_size=per_device_train_batch_size,
        per_device_eval_batch_size=1,   # Nh·ªè ƒë·ªÉ tr√°nh OOM
        gradient_accumulation_steps=gradient_accumulation_steps,
        eval_strategy="no",  # Kh√¥ng eval trong training loop
        save_strategy="epoch",
        save_total_limit=5,
        logging_steps=logging_steps,
        learning_rate=1e-4,
        lr_scheduler_type="cosine",
        warmup_ratio=0.03,
        bf16=True,
        optim="adamw_torch",
        gradient_checkpointing=True,
        max_grad_norm=1.0,
        report_to="none",
        dataloader_pin_memory=False,
        eval_accumulation_steps=1,
        resume_from_checkpoint=has_checkpoint,  # T·ª± ƒë·ªông resume n·∫øu c√≥ checkpoint
    )

    # Apply LoRA to the model
    model = get_peft_model(model, lora_cfg)
    try:
        model.gradient_checkpointing_enable()
    except Exception as e:
        print(f"‚ö†Ô∏è Could not enable gradient checkpointing: {e}")

    # Collator v·ªõi padding ƒë·ªông ƒë·ªÉ tr√°nh l·ªói ƒë·ªô d√†i tensor, ƒë·ªìng th·ªùi g√°n nh√£n -100 cho padding
    collator = DataCollatorForLanguageModeling(tokenizer=tok, mlm=False)

    # Trainer ƒë∆°n gi·∫£n ch·ªâ ƒë·ªÉ train (kh√¥ng eval)
    trainer = Trainer(
        model=model,
        args=args,
        train_dataset=ds_train,
        data_collator=collator,
    )
    
    # Training s·∫Ω t·ª± ƒë·ªông resume t·ª´ checkpoint n·∫øu c√≥
    if has_checkpoint:
        print("üîÑ Resuming training from latest checkpoint...")
    else:
        print("üöÄ Starting training from scratch...")
    
    trainer.train()
    trainer.save_model("out/gemma-vihallu/adapter")
    tok.save_pretrained("out/gemma-vihallu/adapter")
    
    # Gi·∫£i ph√≥ng GPU memory sau training
    torch.cuda.empty_cache()
    
    print("‚úÖ Training completed!")